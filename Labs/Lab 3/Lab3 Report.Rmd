---
title: "Bayesian Learning (732A91) Lab3 Report"
author: "Christoforos Spyretos (chrsp415) & Marketos Damigos (marda352)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
library(ggplot2)
library(mvtnorm)
library(gridExtra)
library(rstan)
```

# Assignment 1 *Gibbs sampler for a normal model*

```{r echo=FALSE}
#____________________________Assignment 1____________________________#

#Reading & preparing data 
precipitation <- as.data.frame(readRDS("Precipitation.rds"))
colnames(precipitation) <- "records"
log_records <- log(precipitation$records)
```

## Task a)

The normal model with conditionally conjugate prior is:

$$
\begin{aligned}
\mu &\sim N(\mu_{0},\tau_{0}^{2}) \\
\sigma^{2} &\sim Inv - \chi^{2}(\nu_{0},\sigma_{0}^{2})
\end{aligned}
$$

The full conditional posteriors are:
$$
\begin{aligned}
\mu|\sigma^{2},x &\sim N(\mu_{n},\tau_{n}^{2}) \\
\sigma^{2}|\mu,x &\sim Inv-\chi^{2}\Big(\nu_{n},\frac{\nu_{0} \sigma_{0}^{2} + \sum_{i=1}^{n}(x_{i}-\mu)^{2}}{n + \nu_{0}}   \Big)
\end{aligned}
$$

Where: 

$$
\begin{aligned}
w & = \frac{\frac{n}{\sigma^{2}}}{\frac{n}{\sigma^{2}} + \frac{1}{\tau_{0}^{2}}} \\
\mu_{n} &= w \overline{x} + (1-w)\mu_{0} \\
\tau_{n}^{2} &= \frac{1}{\frac{n}{\sigma^{2}} + \frac{1}{\tau_{0}^{2}}} \\
\nu_{n} &= \nu_{0} + n
\end{aligned}
$$

```{r echo=FALSE}
# Task 1a
set.seed(1234567890)

# setting up prior parameters
n <- length(log_records)
n_0 <- 1 
mu_0 <- mean(log_records)
sigma2_0 <- var(log_records) 
tau2_0 <- 1

# normal model with conditionally conjugate prior
# mu_prop <- rnorm(n = 1, mean = mu_0, sqrt(tau2_0)) 
# sigma2_prop <-  n_0*sigma2_0 / rchisq(1,n_0)

#sigma2 starting value
sigma2 <- sigma2_0

#vectors to store the samples 
mu_gibbs <- c()
sigma2_gibbs <- c(sigma2)

N <- 1000
for (i in 1:N) {
  
  #calculating parameters 
  w <- (n/sigma2)/((n/sigma2) + (1/tau2_0))
  mu_n <- w * mean(log_records) + (1 - w) * mu_0
  tau2_n <- 1 / (n/sigma2 + 1/tau2_0)
  
  #calculating mu
  mu <- rnorm(n = 1, mean = mu_n, sd = tau2_n)
  
  #temporaly store the previous sigma2
  previous_sigma2 <- sigma2
   
  #calculating parameter
  n_n <- n_0 + n

  #calculating sigma2
  sigma2 <- (n_n*((n_0*sigma2_0+sum((log_records-mu)^2))/ n_n))/rchisq(1,n_n)
  
  #conditions of how to store the samples
  if(i==1){
    mu_gibbs <- c(mu)
    sigma2_gibbs <- c(sigma2)
  }else{
    mu_gibbs <- append(mu_gibbs,c(mu,mu))
    sigma2_gibbs <- append(sigma2_gibbs,c(previous_sigma2,sigma2))
  }
}

```

```{r echo=FALSE}
#calculating autocorrelation for mu
mu_rho <- acf(mu_gibbs,main = "Autocorrelation of mu", col = "red4", lwd = 2)

#callculating the inefficiency factor of mu
IF_mu <- 1+2*sum(mu_rho$acf[-1])

```

The above graph illustrates the correlation coefficient of $\mu$ against the lag. The red bars illustrate the correlation coefficient of $\mu$ of each lag, and the blue lines represent the statistically significant boundaries. The ACF equals 1 for a lag 0, which is expected because the "first step" is always perfectly correlated with itself (the lag 0 autocorrelation is fixed at one by convention). Furthermore, as lag increases, the correlation gets closer to 0, which means the variables are unrelated between them.

```{r echo=FALSE}
#calculating autocorrelation for sigma2
sigma2_rho <- acf(sigma2_gibbs,main = "Autocorrelation of sigma2", col = "red4", lwd = 2)

#callculating the inefficiency factor of sigma2
IF_sigma2 <- 1+2*sum(sigma2_rho$acf[-1])

```

The above graph illustrates the correlation coefficient of $\sigma^{2}$ against the lag. The red bars illustrate the correlation coefficient of $\sigma^{2}$ of each lag, and the blue lines represent the statistically significant boundaries. The ACF equals 1 for a lag 0, which is expected because the "first step" is always perfectly correlated with itself (the lag zero autocorrelation is fixed at one by convention). Furthermore, as lag increases, the correlation gets closer to 0, which means the variables are unrelated between them. Finally, however, the ACF value crosses the "boundaries", which means that the variables are related when lag equals 12, 16 and 30.

https://www.baeldung.com/cs/acf-pacf-plots-arma-modeling \
https://en.wikipedia.org/wiki/Autocorrelation

\

The below table illustrates the inefficiency factors of $\mu$ and $\sigma^{2}$.

```{r echo=FALSE}
#df with the inefficiency factors of mu & sigma
IFs_df <- data.frame("mu"=IF_mu, "sigma2"=IF_sigma2)
rownames(IFs_df)<-c("Inefficiency Factors")
knitr::kable(IFs_df)

```

The inefficiency factor equals 1 if there is not any autocorrelation. The inefficiency factor of $\mu$ is close to 1, which means this is an efficient sample. However, the inefficiency factor of $\sigma^{2}$ equals approximately 1.63, which means that the sample is efficient enough.

```{r echo=FALSE}
#df for plot
plot_df_traj <- data.frame("mu"=mu_gibbs, "sigma2"=sigma2_gibbs)

# plotting some amount of points 
plot_df_traj <- plot_df_traj[1:200,]

#trajectories of the sampled Markov chains.
ggplot(plot_df_traj) +
  geom_path(aes(x = mu, y = sigma2), color = "navy") + 
  geom_point(aes(x = mu[1], y = sigma2[1]), color = "red2", size = 2) +
  geom_point(aes(x = mu[200], y = sigma2[200]), color = "green2", size = 2) +
  ggtitle("Trajectories of the Sampled Markov chains.") + 
  ylab("sigma2") + 
  xlab("mu") +
  theme_classic()

```

The above plot illustrates the trajectories of the first 200 point of the sample, the red dot is the starting point and the green dot is the ending point.

## Task b

```{r echo=FALSE}
#Task b

set.seed(1234567890)

#predicted draws
predictions <- rnorm(nrow(precipitation),mu_gibbs,sqrt(sigma2_gibbs))

#kernel density estimation
density_actual <- density(precipitation$records)
density_pred <- density(exp(predictions))

#df for plot
plot_df_dens <- data.frame( "actual_x" = density_actual$x,
                            "actual_y" = density_actual$y,
                            "pred_x" = density_pred$x,
                            "pred_y" = density_pred$y)

ggplot(plot_df_dens) + 
  geom_line(aes(x=actual_x, y=actual_y), color = "navy") +
  geom_line(aes(x=pred_x, y=pred_y), color = "red2") +
  theme(legend.position="right") +
  scale_color_identity(guide = "legend",
                       name = "",
                       breaks=c("navy", "red2"),
                       labels = c("Actual Values",
                                "Predicted Values")) +
  xlab("Value") +
  ylab("Density") +
  theme_classic()
  
```

# Assignment 2 *Metropolis Random Walk for Poisson regression*

```{r echo=FALSE}
#__________________Assignment 2__________________#

#Reading data 
ebay <- read.table("eBayNumberOfBidderData.dat",header=TRUE)
```

## Task a

```{r echo=FALSE}
# Task a

#glm function, + 0 in order to do not input the covariate Const
model <- glm(formula = nBids ~ 0 + ., data = ebay, family = poisson )

#print the summary of the model 
summary(model)
```

## Task b

Poisson regression model: $y_{i}|beta \sim Poisson[exp(x_{i}^{T}\beta)]$, $i = 1,2,....,n$

The likelihood function of the Poisson regression model is:

$$
L(\theta|X,Y)= \prod_{i}^{n} \frac{e^{Y_{i}\theta^{T}X_{i}}e^{-\theta^{T}X_{i}}}{Y_{i}!}
$$

The log-likelihood of the Poisson regression model is:

$$
l(\theta|X,Y) = log\Big(L(\theta|X,Y)\Big) = \sum_{i}^{n} \Big(Y_{i}\theta^{T}X_{i} - e^{\theta^{T}X{i}} - log(Y_{i}) \Big)
$$

In the above formula, $\theta$ appears in the first two terms; therefore, given that we are only interested in the finding of the best value of $\theta$, we drop the $log(Y_{i})$.  

The final log-likelihood of the Poisson regression model is:

$$
l(\theta|X,Y) = \sum_{i}^{n} \Big(Y_{i}\theta^{T}X_{i} - e^{\theta^{T}X{i}} \Big)
$$

https://en.wikipedia.org/wiki/Poisson_regression

```{r echo=FALSE}
#the below code snippets from a demo of logistic regression in Lecture 6

#target value
y <- ebay$nBids

#prior inputs
# Select which covariates/features to include
X <- as.matrix(ebay[2:10])
Xnames <- colnames(X)

Npar <- dim(X)[2]

# Setting up the prior
mu <- as.matrix(rep(0,Npar)) # Prior mean vector
Sigma <- 100 * solve(t(X)%*%X) # Prior covariance matrix

# Functions that returns the log posterior for the logistic and probit regression.
# First input argument of this function must be the parameters we optimize on,
# i.e. the regression coefficients beta.

LogPostPoisson <- function(betas,y,X,mu,Sigma){
  linPred <- X%*%betas
  #loglikelihood of Poisson regression
  logLik <- sum(linPred*y - exp(linPred))
  if (abs(logLik) == Inf) logLik = -20000 # Likelihood is not finite, stear the optimizer away from here!
  logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE)

  return(logLik + logPrior)
}

# Select the initial values for beta
initVal <- matrix(0,Npar,1)

# The argument control is a list of options to the optimizer optim, where fnscale=-1 means that we minimize
# the negative log posterior. Hence, we maximize the log posterior.
OptimRes <- optim(initVal,LogPostPoisson,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)

```

```{r echo=FALSE, message=FALSE}
names(OptimRes$par) <- Xnames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimRes$hessian))) # Computing approximate standard deviations.
names(approxPostStd) <- Xnames # Naming the coefficient by covariates
#print('The posterior mode is:')
#print(OptimRes$par)
#print('The approximate posterior standard deviation is:')
approxPostStd <- sqrt(diag(-solve(OptimRes$hessian)))
#print(approxPostStd)

```

The below table illustrates the $J_{y}^{-1}(\widetilde{\beta})$.

```{r echo=FALSE}
invHessian <- data.frame("invhes" = -solve(OptimRes$hessian))
colnames(invHessian) <- c("Constant","PowerSeller","VerifyID","Sealed","MinBlem","MajBlem","LargNeg","LogBook","MinBidShare")
knitr::kable(invHessian)

```

The below table illustrates the posterior mode values of every feature of the dataset.

```{r echo=FALSE}
df_post_mode <- data.frame("post_mode" = OptimRes$par)
colnames(df_post_mode) <- c("Value")
rownames(df_post_mode) <- c("Constant","PowerSeller","VerifyID","Sealed","MinBlem","MajBlem","LargNeg","LogBook","MinBidShare")
knitr::kable(df_post_mode)

```

The below table illustrates the approximate posterior standard deviation values of every feature of the dataset.

```{r echo=FALSE}
df_approxPostStd <- data.frame("approxPostStd" =sqrt(diag(-solve(OptimRes$hessian))))
colnames(df_approxPostStd) <- c("Value")
rownames(df_approxPostStd) <-c("Constant","PowerSeller","VerifyID","Sealed","MinBlem","MajBlem","LargNeg","LogBook","MinBidShare")
knitr::kable(df_approxPostStd)
```

## Task c

For the Random walk Metropolis algorithm:

1) Initialize $\theta^{0}$ and iterate for i = 1, 2, ... \

2) Sample proposal: $\theta_{p}|\theta^{(i-1)} \sim N(\theta^{(i-1)}, c \cdot \Sigma)$, where $\Sigma=J_{\hat{\theta},y}^{-1}$ \

3) Compute the acceptance probability: $\alpha = min \Big(1,\frac{p(\theta_{p}|y)}{p(\theta^{(i-1)}|y)}$ , where : \

$\frac{p(\theta_{p}|y)}{p(\theta^{(i-1)}|y)} = exp\Big[log(p(\theta_{p}|y)) - log(p(\theta^{(i-1)}|y))$

4) With probability $\alpha$ set $\theta^{(i)}=\theta_{p}$ and $\theta^{(i)}=\theta^{(i-1)}$

```{r echo=FALSE}
set.seed(1234567890)

RWMSampler <- function(N, logPostFunc, init_beta, constant, cov_mat, target_val, features, mu){
  
  init_sample = rmvnorm(1, init_beta, cov_mat)
  
  #matrix to store the betas 
  betas <- matrix(nrow = N,ncol = length(init_sample))
  
  betas[1,] <- init_sample
  
  for (i in 2:N){
    #sample proposal
    sample_proposal <- as.vector(rmvnorm(1,betas[i-1,], constant*cov_mat))
    
    #LogPostPoisson <- function(betas,y,X,mu,Sigma)
    #calculating the new posterior
    post_new <- logPostFunc(sample_proposal, target_val, features, mu, cov_mat)
    
    #calculating the old posterior
    post_old <- logPostFunc(betas[i-1,], target_val, features, mu, cov_mat)
    
    #acceptance probability
    a <- min(1,exp(post_new - post_old))
    
    u <- runif(1,0,1)
    
    if (u < a){
      betas[i,] <- sample_proposal
    } else{
      betas[i,] <- betas[i-1,]
    }
  }
  return(betas)
}


RWMbetas <- RWMSampler(1000,LogPostPoisson, runif(9,0,1), 0.35, -solve(OptimRes$hessian), ebay$nBids, as.matrix(ebay[2:10]), OptimRes$par)
```

```{r echo=FALSE}
plot_df_RWM <- data.frame(RWMbetas)
colnames(plot_df_RWM) <- c("b1","b2","b3","b4","b5","b6","b7","b8","b9")
plot_df_RWM$iterations <- 1:1000

p1 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b1), color = "navy") + 
  ggtitle("Convergence of b1") +
  xlab("Iterations") +
  ylab("b1") +
  theme_classic()

p2 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b2), color = "navy") + 
  ggtitle("Convergence of b2") +
  xlab("Iterations") +
  ylab("b2") +
  theme_classic()

p3 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b3), color = "navy") + 
  ggtitle("Convergence of b3") +
  xlab("Iterations") +
  ylab("b3") +
  theme_classic()

p4 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b4), color = "navy") + 
  ggtitle("Convergence of b4") +
  xlab("Iterations") +
  ylab("b4") +
  theme_classic()

p5 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b5), color = "navy") + 
  ggtitle("Convergence of b5") +
  xlab("Iterations") +
  ylab("b5") +
  theme_classic()

p6 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b6), color = "navy") + 
  ggtitle("Convergence of b6") +
  xlab("Iterations") +
  ylab("b6") +
  theme_classic()

p7 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b7), color = "navy") + 
  ggtitle("Convergence of b7") +
  xlab("Iterations") +
  ylab("b7") +
  theme_classic()

p8 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b8), color = "navy") + 
  ggtitle("Convergence of b8") +
  xlab("Iterations") +
  ylab("b8") +
  theme_classic()

p9 <-ggplot(plot_df_RWM) +
  geom_line(aes(x=iterations, y=b9), color = "navy") + 
  ggtitle("Convergence of b9") +
  xlab("Iterations") +
  ylab("b9") +
  theme_classic()

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9, ncol=3)
```


## Task d

```{r echo=FALSE}
set.seed(1234567890)

auction <- c(1,1,0,1,0,1,0,1.2,0.8)

nbidders = c()
for(i in 1:nrow(RWMbetas)){
   nbidders[i] <- rpois(1,exp(RWMbetas[i, ] %*% auction))
}

nbidders <- data.frame(nbidders)

prob <- sum(nbidders$nbidders == 0)/(nrow(nbidders))

ggplot(nbidders, aes(x=nbidders)) +
  geom_histogram(bins = 200, color = "navy", fill = "steelblue2")+
  ggtitle("Predictive Distribution") +
  xlab("Number of Bidders") +
  ylab("Density") +
  theme_classic()
```

# Assignment 3 *Time series models in Stan*

## Task a

```{r echo=FALSE}
#__________________Assignment 3__________________#

#Task 3a

set.seed(12345)

#AR(1)-process
ar_process <- function(t,mu,phi,sigma2){
  x <- c()
  x[1] <- mu
  for (i in 2:t){
    x[i] <- mu + phi*(x[i-1]-mu) + rnorm(1,0,sqrt(sigma2))
  }
  return(x)
}

#given parameters
mu <- 13
sigma2 <- 3
t <- 300
phi <- seq(-1,1,0.25)

res <- as.data.frame(sapply(phi, function(phi) ar_process(t,mu,phi,sigma2)))

res$iterations <- 1:t

```

```{r echo=FALSE}
#Time series plots of different phis

p1 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V1), color = "navy") + 
  ggtitle("AR(1)-process phi=-1") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p2 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V2), color = "navy") + 
  ggtitle("AR(1)-process phi=-0.75") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p3 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V3), color = "navy") + 
  ggtitle("AR(1)-process phi=-0.5") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p4 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V4), color = "navy") + 
  ggtitle("AR(1)-process phi=-0.25") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p5 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V5), color = "navy") + 
  ggtitle("AR(1)-process phi=0") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p6 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V6), color = "navy") + 
  ggtitle("AR(1)-process phi=0.25") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p7 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V7), color = "navy") + 
  ggtitle("AR(1)-process phi=0.5") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p8 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V8), color = "navy") + 
  ggtitle("AR(1)-process phi=0.75") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

p9 <-ggplot(res) +
  geom_line(aes(x=iterations, y=V9), color = "navy") + 
  ggtitle("AR(1)-process phi=1") +
  xlab("Iterations") +
  ylab("x") +
  theme_classic()

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9, ncol=3)

```

## Task b 

```{r echo=FALSE}
# Task 3b

set.seed(1234567890)

#stan_model
StanModel = '
data {
  int<lower=0> N; 
  vector[N] x;
}
parameters {
  real mu;
  real<lower=0> sigma2;
  real phi;
}
model {
  mu ~ normal(0,100); // Normal with mean 0, st.dev. 100
  sigma2 ~ scaled_inv_chi_square(1,2); // Scaled-inv-chi2 with nu 1,sigma 2
  for(i in 2:N){
    x[i] ~ normal( mu + phi*(x[i-1]-mu), sqrt(sigma2));
  }
}'

#given parameters 
phi1 <- 0.2
phi2 <- 0.95

#ar with given parameters
res1 <- ar_process(t,mu,phi1,sigma2)
res2 <- ar_process(t,mu,phi1,sigma2)

data1 <- list(N=t, x=res1)
data2 <- list(N=t, x=res2)

warmup <- 1000
niter <- 2000


fit1 <- stan(model_code=StanModel,data=data1, warmup=warmup,iter=niter, chains = 4)
fit2 <- stan(model_code=StanModel,data=data2, warmup=warmup,iter=niter, chains = 4)

```

## i)

```{r echo=FALSE}
fit1_summary <- summary(fit1)
phi1_stats <- fit1_summary$summary[1:3,c(1,3,4,8,9)]

knitr::kable(phi1_stats,caption = "Table for phi=0.2")

```

```{r echo=FALSE}
fit2_summary <- summary(fit2)
phi2_stats <- fit2_summary$summary[1:3,c(1,3,4,8,9)]

knitr::kable(phi2_stats,caption = "Table for phi=0.95")

```

## ii)

```{r echo=FALSE}
postDraws1 <- data.frame(extract(fit1))
postDraws1$iterations <- 1:4000

ggplot(postDraws1) +
  geom_line(aes(x=iterations, y=mu), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.2") +
  xlab("Iterations") +
  ylab("mu") +
  theme_classic()

ggplot(postDraws1) +
  geom_line(aes(x=iterations, y=sigma2), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.2") +
  xlab("Iterations") +
  ylab("sigma2") +
  theme_classic()

ggplot(postDraws1) +
  geom_line(aes(x=iterations, y=phi), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.2") +
  xlab("Iterations") +
  ylab("phi") +
  theme_classic()

ggplot(postDraws1) +
  geom_point(aes(x=mu, y=phi), color ="navy") +
  ggtitle("Joint Posterior phi=0.2") +
  theme_classic()
```

```{r echo=FALSE}
postDraws2 <- data.frame(extract(fit2))
postDraws2$iterations <- 1:4000

ggplot(postDraws2) +
  geom_line(aes(x=iterations, y=mu), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.95") +
  xlab("Iterations") +
  ylab("mu") +
  theme_classic()

ggplot(postDraws2) +
  geom_line(aes(x=iterations, y=sigma2), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.95") +
  xlab("Iterations") +
  ylab("sigma2") +
  theme_classic()

ggplot(postDraws2) +
  geom_line(aes(x=iterations, y=phi), color ="navy") +
  ggtitle("Convergence Plot of mu with phi=0.95") +
  xlab("Iterations") +
  ylab("phi") +
  theme_classic()

ggplot(postDraws2) +
  geom_point(aes(x=mu, y=phi), color ="navy") +
  ggtitle("Joint Posterior phi=0.95") +
  theme_classic()
```


# *Appendix*
```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```